{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05a519",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Phase 5: Feature Engineering + Modeling Prep\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load enriched fraud dataset from Phase 3 (with country and time features)\u001b[39;00m\n\u001b[32m      9\u001b[39m fraud_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mdata/raw/fraud_data.csv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# replace with actual merged file if you saved earlier\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Phase 5: Feature Engineering + Modeling Prep\n",
    "# ----------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load enriched fraud dataset from Phase 3 (with country and time features)\n",
    "fraud_df = pd.read_csv(\"data/raw/fraud_data.csv\")  # replace with actual merged file if you saved earlier\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "\n",
    "# Create time difference (hours) between signup and purchase\n",
    "fraud_df['time_diff'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Extract hour of purchase and day of week\n",
    "fraud_df['purchase_hour'] = fraud_df['purchase_time'].dt.hour\n",
    "fraud_df['purchase_day'] = fraud_df['purchase_time'].dt.day_name()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1️⃣ Select meaningful columns\n",
    "# ----------------------------------------\n",
    "model_df = fraud_df[[\n",
    "    'purchase_value',\n",
    "    'device_id',\n",
    "    'source',\n",
    "    'browser',\n",
    "    'sex',\n",
    "    'age',\n",
    "    'ip_address',\n",
    "    'country',\n",
    "    'time_diff',\n",
    "    'purchase_hour',\n",
    "    'purchase_day',\n",
    "    'class'\n",
    "]].copy()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2️⃣ Handle missing values\n",
    "# ----------------------------------------\n",
    "model_df['age'] = model_df['age'].fillna(model_df['age'].median())\n",
    "model_df['country'] = model_df['country'].fillna(\"Unknown\")\n",
    "model_df.dropna(inplace=True)  # drop rows with any remaining missing\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3️⃣ Feature Engineering\n",
    "# ----------------------------------------\n",
    "\n",
    "# A) Very fast signup → purchase\n",
    "model_df['instant_purchase'] = (model_df['time_diff'] < 1).astype(int)\n",
    "\n",
    "# B) Night transaction flag (23:00-05:00)\n",
    "model_df['is_night'] = model_df['purchase_hour'].apply(lambda x: 1 if x >= 23 or x <= 5 else 0)\n",
    "\n",
    "# C) High purchase value\n",
    "median_purchase = model_df['purchase_value'].median()\n",
    "model_df['high_value'] = (model_df['purchase_value'] > median_purchase).astype(int)\n",
    "\n",
    "# D) Risk country flag (top 3 fraud countries)\n",
    "risky_countries = model_df[model_df['class']==1]['country'].value_counts().head(3).index\n",
    "model_df['risk_country'] = model_df['country'].isin(risky_countries).astype(int)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4️⃣ Encode categorical features\n",
    "# ----------------------------------------\n",
    "\n",
    "# Drop high cardinality device_id\n",
    "model_df = model_df.drop(columns=['device_id', 'ip_address'])\n",
    "\n",
    "# Categorical columns to one-hot encode\n",
    "cat_cols = ['source', 'browser', 'sex', 'country', 'purchase_day']\n",
    "model_df = pd.get_dummies(model_df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5️⃣ Scale numeric features\n",
    "# ----------------------------------------\n",
    "num_cols = ['purchase_value', 'age', 'time_diff', 'purchase_hour']\n",
    "scaler = StandardScaler()\n",
    "model_df[num_cols] = scaler.fit_transform(model_df[num_cols])\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6️⃣ Save processed dataset\n",
    "# ----------------------------------------\n",
    "model_df.to_csv(\"data/processed/fraud_data_processed.csv\", index=False)\n",
    "print(\"Processed fraud dataset saved to data/processed/fraud_data_processed.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7️⃣ Optional: Quick train/test split for modeling prep\n",
    "# ----------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_df.drop(columns=['class'])\n",
    "y = model_df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Test split complete.\")\n",
    "print(\"Train size:\", X_train.shape[0], \"Test size:\", X_test.shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
